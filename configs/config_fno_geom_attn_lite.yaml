# Lightweight FNO with Geometry Cross-Attention
# Memory-efficient version: ~2-3GB (vs 12GB for full version)
#
# Key optimizations:
# - 54 geometry tokens (vs 432)
# - 32-dim tokens (vs 64)
# - Attention only at layers 4,5 (vs all 6)
# - Single-layer geometry encoder

data:
  data_dirs:
    - data
    - data/downsampled_highres
  data_dir: data
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  batch_size: 1
  num_workers: 0
  pin_memory: true

model:
  backbone: fno_geom_attn_lite
  add_analytical_solution: true
  geometry_encoder:
    enabled: true
    hidden_dim: 64
    num_layers: 2
  fno:
    modes1: 8
    modes2: 8
    modes3: 8
    width: 32
    num_layers: 6
    fc_dim: 128
  # Lightweight geometry attention config
  geometry_attention:
    in_channels: 7          # 6 conductivity + 1 mask
    token_dim: 32           # Reduced from 64
    num_tokens: [3, 3, 6]   # 54 tokens (vs 432)
    num_heads: 2            # Reduced from 4
    attention_layers: [4, 5]  # Only last 2 layers

training:
  epochs: 100
  learning_rate: 0.001
  weight_decay: 0.00001
  scheduler: cosine
  scheduler_params:
    T_max: 100
    eta_min: 0.000001
  checkpoint_dir: checkpoints
  checkpoint_freq: 10
  resume_from: null
  early_stopping_patience: 10
  early_stopping_delta: 0.000001

loss:
  pde_weight: 0.0
  tv_weight: 0.01
  gradient_matching_weight: 0.0

spacing:
  use_spacing_conditioning: true

experiment:
  use_singularity_mask: true

grid:
  base_resolution: [96, 48, 48]
  train_resolution: [96, 48, 48]
  test_resolution: [96, 48, 48]
  log_conductivity: false
  conductivity_epsilon: 0.000001

physics:
  analytical:
    epsilon_factor: 0.1
    current_I: 1.0
  pde:
    conductivity_threshold: 0.000001
  mse:
    singularity_mask_radius: 3

logging:
  use_wandb: false
  use_tensorboard: true
  log_dir: logs
  project_name: neural-operator-3d
  experiment_name: fno_geom_attn_lite
  vis_freq: 10
  vis_slices: [axial, sagittal]
  vis_dir: visualizations

evaluation:
  resolution_invariance_test: true
  test_resolutions:
    - [192, 96, 96]
    - [96, 48, 48]
